{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2fwdrJCyk73",
    "outputId": "80ecaca9-9eda-40fa-f71c-ad473cfc6513"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floor\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     47\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0,1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import seaborn as sns \n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import norm\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostRegressor, Pool, metrics, cv\n",
    "import xgboost as xgb\n",
    "from scipy.stats import gmean\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import datetime\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "print(\"tf version:\",tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Dense, Conv2D, LSTM, Flatten, Reshape, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waJniYqiypEb"
   },
   "outputs": [],
   "source": [
    "global_region_tensor = np.load('./train_global_region_tensor_scaled_sg5.npy', allow_pickle=True)\n",
    "column_df = pd.read_csv('./column_names.csv')\n",
    "region_df = pd.read_csv('./target_points.csv')\n",
    "col_names = column_df['name']\n",
    "target_regions = region_df['region_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA8MHBZ34KfI"
   },
   "outputs": [],
   "source": [
    "temporal_attrs = ['year', 'month', 'dayofyear', 'season', 'day_of_year_sin', 'day_of_year_cos','month_sin', 'month_cos', 'season_sin', 'season_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRhVrAc63Rv3",
    "outputId": "181d350e-9bbb-45cf-c19a-cc2f62e41247"
   },
   "outputs": [],
   "source": [
    "print(\"Global region tensor shape:\", global_region_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVqHtuhD4W1f",
    "outputId": "4ae6c473-e6cb-45d0-b3c6-7d3a1ef5da0d"
   },
   "outputs": [],
   "source": [
    "target_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWupitGU5gsT",
    "outputId": "40f5d145-ed28-4ed0-ce98-69f9d815e503"
   },
   "outputs": [],
   "source": [
    "region_df['lon_index'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YotbwGa73OVo"
   },
   "outputs": [],
   "source": [
    "MAX_SG = 5\n",
    "NUM_TIMESTEPS = global_region_tensor.shape[0]\n",
    "NUM_REGIONS = len(target_regions)\n",
    "NUM_FEATURES = global_region_tensor.shape[3]\n",
    "TEMPORAL_FEATURES = len(temporal_attrs)\n",
    "SPATIAL_FEATURES = NUM_FEATURES-TEMPORAL_FEATURES - 3 - 1\n",
    "PADDED_LAT = global_region_tensor.shape[1]\n",
    "PADDED_LON = global_region_tensor.shape[2]\n",
    "TRAIN_SPLIT = NUM_TIMESTEPS - 2*30\n",
    "DROPOUT_RATE = 0.3\n",
    "VALID_SPLIT = NUM_TIMESTEPS - 30\n",
    "SG = 5\n",
    "SPATIAL_WIDTH = 2*SG+1\n",
    "target_region_ids = []\n",
    "\n",
    "for region in target_regions:\n",
    "  tmp_list = region.split(', ')\n",
    "  tmp_list = [_.strip() for _ in tmp_list]\n",
    "  region_0 = int(tmp_list[0][1:])\n",
    "  region_1 = int(tmp_list[1][:-1])\n",
    "  a = region_df['lat_index'].max()-region_0+MAX_SG\n",
    "  b = region_1-region_df['lon_index'].min()+MAX_SG\n",
    "  target_region_ids.append((a, b))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 15000\n",
    "PREFETCH_SIZE = 5\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABuHLErQ43M6"
   },
   "outputs": [],
   "source": [
    "CONV2D_LAYERS = 1\n",
    "CONV1D_LAYERS = 0\n",
    "RGN_ID_EMB_DIM = 8\n",
    "SPTL_CONV_FILTERS = 16\n",
    "SPTL_KERNEL_SIZE = 2\n",
    "SPTL_STRIDE = 1\n",
    "SPTL_PADDING = 'same'\n",
    "SPTL_OUTPUT_DIM = 24\n",
    "CONV1D_FILTERS = 16\n",
    "REC_DROPOUT_RATE = 0\n",
    "FC1_UNITS = 128\n",
    "FC2_UNITS = 128\n",
    "PRED_TAR = 'tmp2m'\n",
    "TAR_VAR = 'tmp2m'\n",
    "MODEL_NAME = PRED_TAR+'_sp-tm_sg'+str(SG)+'_emb'+str(RGN_ID_EMB_DIM)\n",
    "TIMESTAMP = '{date:%Y_%m_%d_%H_%M}'.format(date = datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4rbe0MP9MXD"
   },
   "outputs": [],
   "source": [
    "rgn_id_vocab = [str(region) for region in target_regions]\n",
    "rgn_id_to_int = {rgn_id:i for i, rgn_id in enumerate(rgn_id_vocab)}\n",
    "int_to_rgn_id = {i:rgn_id for i, rgn_id in enumerate(rgn_id_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKjkNxk49gqx",
    "outputId": "97d1656b-43ae-4ae8-b28d-98129dab2d4b"
   },
   "outputs": [],
   "source": [
    "print(\"Prediction target: \",PRED_TAR,\n",
    "      \"\\nEpochs: \",EPOCHS,\n",
    "      \"\\nBatch size: \",BATCH_SIZE,\n",
    "      \"\\nSpatial granularity: \", SG,\n",
    "      \"\\nMax spatial granularity: \", MAX_SG,\n",
    "      \"\\nModel name: \", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mp5ioRnNL3U"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./n_train.csv')\n",
    "test_df = pd.read_csv('./n_test.csv')\n",
    "train_all_date_list = train_df['startdate'].unique()\n",
    "test_all_date_list = test_df['startdate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8xLwc99Nfza"
   },
   "outputs": [],
   "source": [
    "locations_df = pd.read_csv('./locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMT-y6slOSpC"
   },
   "outputs": [],
   "source": [
    "target = 'contest-tmp2m-14d__tmp2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wW2daVb7gDCZ",
    "outputId": "5f75fae0-56cd-499d-9696-ab297bb59780"
   },
   "outputs": [],
   "source": [
    "lat_max = locations_df['lat_index'].max()\n",
    "lon_min = locations_df['lon_index'].min()\n",
    "print(lat_max, lon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "daGelpnYhwpy",
    "outputId": "e1154d17-1182-4e6d-dae0-70d8f3206a59"
   },
   "outputs": [],
   "source": [
    "region_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "rVJH-d4DitQ2",
    "outputId": "fbd423e5-c7e0-4caf-e249-402165829e90"
   },
   "outputs": [],
   "source": [
    "location = locations_df[(locations_df['lat_index'] == 47) & (locations_df['lon_index'] == 268)]\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "9g2oAbPWi0K3",
    "outputId": "1d57914e-1746-47c8-b51a-7fe8bc821a2b"
   },
   "outputs": [],
   "source": [
    "train_df[(train_df['loc_group'] == 456) & (train_df['startdate'] == '2016-01-19')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQCX7GSAIK9i"
   },
   "outputs": [],
   "source": [
    "def get_target_data_by_date_index_and_region(target_region, date):\n",
    "  # print(target_region, lat_max + MAX_SG - target_region[0], target_region[1] + lon_min - MAX_SG, date)\n",
    "  location = locations_df[(locations_df['lat_index'] == lat_max + MAX_SG - target_region[0]) & (locations_df['lon_index'] == target_region[1] + lon_min - MAX_SG)]\n",
    "  if len(location) == 0:\n",
    "    return 0\n",
    "  loc_group = location['loc_group'].tolist()[0]\n",
    "  # print(target_region, lat_max + MAX_SG - target_region[0], target_region[1] + lon_min - MAX_SG, date, loc_group)\n",
    "  sample = train_df[(train_df['loc_group'] == loc_group) & (train_df['startdate'] == date)]\n",
    "  # print(sample)\n",
    "  return sample[target].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvt_oh3wt-bp"
   },
   "outputs": [],
   "source": [
    "t = [0 for i in range(68)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtsy_2IYt2Tk",
    "outputId": "66faf0ee-c0d5-48e6-f93c-2a86f0287726"
   },
   "outputs": [],
   "source": [
    "len(t[3:-TEMPORAL_FEATURES - 1])\n",
    "len(t[-TEMPORAL_FEATURES - 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbM2mKc59lqh"
   },
   "outputs": [],
   "source": [
    "def window_single_spatial_temporal_target(dataset, target_region, index, sg, date_list, seq_len, tar_var=TAR_VAR,\n",
    "                                           region_emb=True):\n",
    "\n",
    "    local_region = dataset[:, (target_region[0] - sg):(target_region[0] + sg + 1),\n",
    "                   (target_region[1] - sg):(target_region[1] + sg + 1), :]\n",
    "    \n",
    "\n",
    "    spatial_data = local_region[[index], :, :, 3:-TEMPORAL_FEATURES - 1].astype(np.float16)\n",
    "\n",
    "    if region_emb:\n",
    "        local_reg_id = str((int(local_region[0, sg, sg, 1]), int(local_region[0, sg, sg, 2])))\n",
    "        local_reg_id = rgn_id_to_int[local_reg_id]\n",
    "        temporal_data = local_region[[index], sg, sg, -TEMPORAL_FEATURES - 1:-1].astype(np.float16)\n",
    "        region_embedding = np.repeat(local_reg_id, 1).reshape(1, 1).astype(np.int16)\n",
    "    target_data = get_target_data_by_date_index_and_region(target_region, date_list[index])\n",
    "    # print(target_region, date_list[index], target_data)\n",
    "\n",
    "    return np.array(spatial_data), np.array(temporal_data), np.array(region_embedding), np.array(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZeTE26_ER4N"
   },
   "outputs": [],
   "source": [
    "def generate_input_batch(dataset, batch_size, sg,  tar_var, date_list, seq_len):\n",
    "\n",
    "    spatial_data, temporal_data, reg_emb_data, target_data = [], [], [], []\n",
    "    for batch in range(batch_size):\n",
    "        rand_region_idx = np.random.choice(NUM_REGIONS, 1, replace=False)[0]\n",
    "        rand_region = target_region_ids[rand_region_idx]\n",
    "        rand_date_idx = np.random.choice(NUM_TIMESTEPS, 1, replace=False)[0]\n",
    "        spt, tmp, remb, tar = window_single_spatial_temporal_target(dataset, rand_region, rand_date_idx, sg, date_list,\n",
    "                                                                     seq_len, tar_var)\n",
    "        spatial_data.append(spt)\n",
    "        temporal_data.append(tmp)\n",
    "        reg_emb_data.append(remb)\n",
    "        target_data.append(tar)\n",
    "\n",
    "    spatial_data = np.stack(spatial_data)\n",
    "    temporal_data = np.stack(temporal_data)\n",
    "    reg_emb_data = np.stack(reg_emb_data)\n",
    "    target_data = np.stack(target_data)\n",
    "\n",
    "    # yield  ({\"spatial_input\": tf.convert_to_tensor(spatial_data), \"temporal_input\": tf.convert_to_tensor(temporal_data), \"region_id_input\": tf.convert_to_tensor(reg_emb_data)},\n",
    "    #        {\"target_output\": tf.convert_to_tensor(target_data)})\n",
    "\n",
    "    yield ({\"spatial_input\": spatial_data, \"temporal_input\": temporal_data, \"region_id_input\": reg_emb_data},\n",
    "           {\"target_output\": target_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_YD2ogbs3OL"
   },
   "outputs": [],
   "source": [
    "res = generate_input_batch(dataset=global_region_tensor, batch_size=BATCH_SIZE, sg=SG, tar_var=TAR_VAR, date_list= train_all_date_list, seq_len=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zKqJVAOQP0D"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 1\n",
    "dataset_generator = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: generate_input_batch(dataset=global_region_tensor, batch_size=BATCH_SIZE, sg=SG, tar_var=TAR_VAR, date_list= train_all_date_list, seq_len=SEQ_LEN),\n",
    "    output_types=({\"spatial_input\":np.float64,\"temporal_input\":np.float64,\"region_id_input\":np.int16},{\"target_output\":np.float64}),\n",
    "    output_shapes=({\"spatial_input\":[BATCH_SIZE,SEQ_LEN, SPATIAL_WIDTH,SPATIAL_WIDTH,SPATIAL_FEATURES], \\\n",
    "                    \"temporal_input\":[BATCH_SIZE,SEQ_LEN, TEMPORAL_FEATURES],\n",
    "                    \"region_id_input\":[BATCH_SIZE,SEQ_LEN, 1]}, \\\n",
    "                    {\"target_output\":[BATCH_SIZE,]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ou11DNkktEh"
   },
   "outputs": [],
   "source": [
    "dataset_generator = dataset_generator.prefetch(PREFETCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzfvjZu3k8ov",
    "outputId": "92695537-38fa-494a-f6e8-dd82e7e70c74"
   },
   "outputs": [],
   "source": [
    "def build_sptl_tmpl_model(sptl_width=SPATIAL_WIDTH, sptl_features=SPATIAL_FEATURES,\n",
    "                          tmpl_features=TEMPORAL_FEATURES, batch_size=BATCH_SIZE,\n",
    "                          sptl_conv_filters=SPTL_CONV_FILTERS, sptl_kernel_size=SPTL_KERNEL_SIZE,\n",
    "                          sptl_stride=SPTL_STRIDE, sptl_padding=SPTL_PADDING, sptl_output_dim=SPTL_OUTPUT_DIM,\n",
    "                          rgn_id_vocab=NUM_REGIONS, rgn_id_emb_dim=RGN_ID_EMB_DIM, dropout_rate=DROPOUT_RATE, model_name=MODEL_NAME,\n",
    "                          print_summary=True):\n",
    "\n",
    "    spatial_input = Input(shape=[1, sptl_width, sptl_width, sptl_features], batch_size=batch_size,\n",
    "                          name='spatial_input')\n",
    "\n",
    "    spatial_model = Conv2D(filters=sptl_conv_filters, kernel_size=(sptl_kernel_size, sptl_kernel_size),\n",
    "                                           strides=(sptl_stride, sptl_stride), padding=sptl_padding,\n",
    "                                           name='sptl_conv2d')(spatial_input)\n",
    "\n",
    "    spatial_model = Flatten(name='sptl_flatten')(spatial_model)\n",
    "\n",
    "    spatial_model = Reshape(target_shape=(1, -1), name='sptl_reshape')(spatial_model)\n",
    "\n",
    "    spatial_output = Dense(units=sptl_output_dim, activation='relu', name='sptl_emb_out')(spatial_model)\n",
    "\n",
    "    # Temporal Model Input\n",
    "    temporal_input = Input(shape=(1, tmpl_features), batch_size=batch_size, name='temporal_input')\n",
    "\n",
    "    # Region ID Input\n",
    "    region_id_input = Input(shape=(1, 1), batch_size=batch_size, name='region_id_input')\n",
    "\n",
    "    region_id_emb = Reshape((1,), input_shape=(1, 1), name='region_id_reshape')(region_id_input)\n",
    "\n",
    "    region_id_emb = Embedding(input_dim=rgn_id_vocab, input_length=1, mask_zero=False, batch_size=batch_size,\n",
    "                              output_dim=rgn_id_emb_dim, name='durations_emb_layer')(region_id_emb)\n",
    "\n",
    "    # Concat Inputs\n",
    "    concat = K.concatenate([spatial_output, temporal_input, region_id_emb], axis=-1)\n",
    "\n",
    "    # LSTM Architecture\n",
    "    # lstm_model = LSTM(units=lstm1_units, return_sequences=True, name='lstm_1')(concat)\n",
    "\n",
    "    # lstm_model = LSTM(units=lstm2_units, return_sequences=False, name='lstm_2')(lstm_model)\n",
    "\n",
    "    model_output = Dense(units=1, name='target_output')(concat)\n",
    "\n",
    "    model = Model(inputs=[spatial_input, temporal_input, region_id_input], outputs=[model_output], name=model_name)\n",
    "\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    return (model)\n",
    "\n",
    "spatial_temporal_model = build_sptl_tmpl_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eASlkt2rnW6S",
    "outputId": "87c5c7fb-5899-4501-a3e0-9036a3a5762d"
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = int(len(train_df) / BATCH_SIZE)\n",
    "STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2Apyxy7muv1",
    "outputId": "651091ef-ccea-4ad8-d7e4-a13d612b6baa"
   },
   "outputs": [],
   "source": [
    "RUN_ID = \"1\"\n",
    "def train_spatial_temporal_model(model, dataset_generator, opt='adam', epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                 include_tb=False): \n",
    "\n",
    "    ## Early stopping\n",
    "    earlystopping = EarlyStopping(monitor='loss', min_delta=0.00001, patience=10, restore_best_weights=True)  # val_loss\n",
    "\n",
    "    # Automatically save latest best model to file\n",
    "    filepath =  \"./models/model_saves/\" + PRED_TAR + '/' + RUN_ID + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "    # Set callbacks\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # Optimizers\n",
    "    optimizers = {'adam': Adam(learning_rate=0.03, beta_1=0.9, beta_2=0.999, amsgrad=False)}\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizers[opt],\n",
    "                  metrics=[RootMeanSquaredError(), Huber()])\n",
    "\n",
    "    # Fit model #x = [spatial_train, temporal_train_x], y = temporal_train_y,\n",
    "    history = model.fit(dataset_generator, epochs=epochs, use_multiprocessing=True,\n",
    "                        steps_per_epoch=steps_per_epoch, verbose=1, callbacks=callbacks_list)\n",
    "    return (history)\n",
    "\n",
    "## Train ##\n",
    "history = train_spatial_temporal_model(model = spatial_temporal_model, dataset_generator = dataset_generator,\n",
    "                                       opt = 'adam', steps_per_epoch=500, include_tb=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
